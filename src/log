
Tue Jun  7 15:49:02 CST 2016
* 关于硬盘head number的疑问（网上的)
  。。从以上可以看出/dev/sda这块硬盘有255个head，以我对硬盘构造的了解，一个硬盘是由若干同心圆的platter组成，每个platter都有一个或两个磁头（单面或双面）用来读写platter上的数据。磁盘那么小一个，怎么可能有255个磁头？
  。。。。>
  我同事说那个磁头是不真实的，是模拟出来的，现在LBA寻址，磁头数都是模拟出来的。
  。。。>
  楼主要想知道为什么，去看看LBA的知识吧，上面现实的都是LBA的结果，不是实际的磁头。。
Tue Jun  7 15:06:30 CST 2016
× 关于vendor information 
  主要是纠结vendor infomation list太大的问题。
1, 我从网上找到了一个C语言头文件形式的，但还是太大，500K的大小，用ascii编码也要250K,再算1/2的字符串量，也要120K.除去里面的杂的内容，也要100K。
2， 搜到的那个头文件，它的组织已经接近正确了。 因为vendor不是在0~65535分布满的，必须要这样才能密集存储。
3， 我在pci_vendor.c里，用start_of_vseg把PciDevTable这个大表分段，Make Fast Access。
   那样存储的还有一个好处是，在小设备上，裁剪内核的时候，可以把那个Table裁剪的很小。
4， 我觉得这样可以了，但突然想到一个问题。 像papaya这样的内核，它很长一段时间以内都会是几百KB这样的样子，拿100多k来装字符串，我觉得有些别扭。  所以决定是内核存储的信息不细化到device。
  算了算了，还是暂时先放到内核里。100K，还能忍受吧。
  。。。还是从内核中分离出去把，因为目前来看，内存印象只在1M内存里，而且要做elf的解析和装载，就只能用一半了，内核的大小大概也就60000~a00000,240K的限制了。暂时还不想动这一块。所以还是尽量控制内核映像的大小。
  单这样以来，内核印象的运行就需要文件系统，因为映像里一些大的信息部分被当做文件分离出去。

Tue Jun  7 14:32:54 CST 2016
1, boot.asm里又一个old bug被发现了。 在	int 15probe内存的时候，把jc写成了je。jc跟je其实没关系！ je只跟zf标志有关。
Tue Jun  7 13:21:18 CST 2016
*  用08号测parameter，得到400m.img的sector_per是63，heads=(63-48+1)
  在裸机上测的  h.d1s?  即heads=46 disk=2 sector_per=63
* 就保证kernel.elf不超过60kb，这样总共只需要读两个track。我不知道第三个track是在哪个side。

Mon Jun  6 22:38:46 EDT 2016
× 开始裸机测试。
  连kernel.asm都没能进去。问题出在往0xc00304xx跳过去后失败了。 
  那里没有指令。
  kernel reset没问题，但kernel reset的source就么有指令。
  因为用dap读磁盘失败了。虽然显示的是成功。
*  关于es:di, ds:si 
  一直以来都以为[di]默认是用es做为段寄存器，今天才酿成了祸。。。。记住，只要不指定段寄存器，像这样[],都是默认ds的。 
  之所以有这个错误印象，是因为rep时，是默认ds:si => es:di的。

Sun Jun  5 23:07:13 EDT 2016
* 开始做网卡驱动了。 rtl8139的datasheet很简略，我一直怀疑realtek公司应该有xx guide一类的文档存在。 在google上搜rtl8139 guide，竟然找到了。
  这篇文档看的我泪流满面，每句话都说到我心里，我想问的像听的，都在上面，没有一句废话。
  文档的年份是1999,1,15  RTL8139(A/B) Programming guide: (V0.1)  来源是usfca.edu.com，美国旧金山大学。 我竟然有兴趣搜了搜这个学校怎么样。 

* CAPR是干什么的。 它是由驱动更新的，那网卡硬件要它何用?
  网摘：收报处理最后一部是更新读指针，包括主机CPU副本cur_rx和8139的CAPR。更行8139的CAPR还有一个重要的副作用，就是如果CAPR=CBA时，8139置CR.RxBufEmpry为1，表示缓冲为空，8139对外发出取消暂停控制帧。。。


Sat Jun  4 22:32:15 EDT 2016
* 再确定一遍。控制mmzone的gfp_flags是在kmem_cache_create时候就确定下来了。 以后kmem_cache_alloc时候，还会传gfp_flags，但那个就是__GFP_KERNEL或者__GFP_ATOMIC。grow时候，alloc_pages时候，用到的flags是slabhead->gfpflags | arg_flags。

Fri Jun  3 21:32:30 EDT 2016
* 从SET_PAGE_slab()到page_set_slab()的变迁
  当一个page被分配给slab模块后，就在page结构体内标记上它所属的slab和slab_head。2.4内核里，标记slab是通过SET_PAGE_SLAB，一个宏，具体源码我没看，但是从《情景》的介绍来看，这个宏是把slab链到page->list->prev。
  到了2.6，有两个改变。第一是函数命名的调整，page_set_slab更符合我的口味.第二是把slab链到lru->prev上，其实新版本里的struct page里已经不再有list这个链接点了。属于内核的变动。
  这个函数的实现也很喜欢：
	static inline void page_set_slab(struct page *page, struct slab *slab)
	{
		page->lru.prev = (struct list_head *)slab;
	}
  在papaya里，我把这个函数命名成page_mark_slab。

* 通过kmem_cache_create()创建的专用slab队列，在创建时，就要指定是否是DMA类型的。它里面的所有slab，要么全在DMA内存区(<16M),要么全不在。
  但通用slab队列(通过kmalloc分配)，则是在alloc时，指定是否是DAM页面。 generic cache本身就维护了两个slab队列。（一个是DMA，一个不是).
 
* makefile的维护有点吃力了
  像mm,fs这些目录下的makefile，都是只依赖于c文件，.h文件的更新不会引发重编译。 所以今天slab.c里的一个函数找不到链接，我去utils.h里加了个static inilne的，make，还是提示链接不到。运行make clean，也不行。 似乎必须把.c文件保存了才行。 其实make clean之后按理该重编译的，但因为mm文件夹是新添加的，clean的时候，不会清除其中的.o文件。 必须手动添加相应的命令在总makefile里。

Thu Jun  2 21:33:02 EDT 2016
*slab的freelist数组是紧跟在slab结构体的后面，这是很常见的，因为freelist长度不固定，所以放在结构体尾巴上。 但因此有一个问题，若遇到1/4,1/2 page_sze的objsize，slab结构体要分离出slab，那给slab结构体分配内存空间就要小心， 它不是固定的大小！那它就没有专用的slab队列了。不知道linux怎么做的。 我暂时不考虑“游离”slab_s的情况。
* cache_cache不仅通过partial,unsedout,fresh三个指针维持着一个普通的slab_head类型的slab队列，还通过global持有所有正在纸用的slab_head。

Mon May  2 01:21:10 EDT 2016
*vfs层还比较薄弱。 文件的缓冲迫切需要做上。 这依赖于块设备层的缓冲。
*慢慢来，任何模块都不能生疏，不能做完就放下，如果总是很快就忘，说明那个模块的设计有问题。
 

Sat Apr 30 23:25:41 EDT 2016
*一个进程使用了进程号9，另一个进程open还可能得到9这个fd吗。fd是不是全局的?那就必然要有一个全局的fd table。
Fri Apr 29 14:16:17 EDT 2016
*一个目录里有许多dentry，删除了一些之后，这些空置的dentry就以碎片的形式存在。现在有两个问题：1， 怎么在需要的时候找出合适的，怎么找出最合适的？2，一个长度为16的dentry被用于存储10的name，那么剩余的6个字节就永远不能寻回了。。。不对不对，是根本就不能存储非16长度的name，因为还要用这个16定位下一个dentry。 我现在想到的方法，是定期的做碎片整理。时机可以选在系统空闲的时候，或者一个dentry block将满的时候。

Fri Apr 29 06:24:59 EDT 2016
*刚才为inode_hashtable和dentry_hashtable申请内存时，调用的参数是(4 * D_HASHTABLE_LEN)和(4 * I_HASHTABLE_LEN),但hashtable不是一个指针数组，结果内存越界，出现handle_irq_event里的“sti now"的bug。以后要小心了。 申请内存和初始化这些东西，一定要小心，内存越界的bug排查起来会很辛苦。

Fri Apr 29 04:08:42 EDT 2016
*必须等init进程加载完分区表，才能创建别的进程。因为就算init vfs，也要等到分区表建立完成。而读分区表（包括ebr）是一个异步的，需要一段时间。（不如就把init vfs放在init 进程的后面，不要fun0,本来就该)

*各个挂载设备的root dentry会放在hash table里吗，如果是，他们的hash值是完全相同的。就没办法在链表里区分了。---------------不会，它们只是由vfsmount->small_root指向。

Thu Apr 28 23:02:24 EDT 2016
*对于总root，不仅挂载设备的root dentry是虚拟的，而且挂载点是也是虚拟的。 似乎可以不要这个挂载点。。不是。我们要让根设备看起来是挂载在"/"上的。
* 我在考虑不用挂载的方式实现总root。 但还是不能，因为这是整个文件系统的第一个vfsmount结构，子级的vfsmount要通过parent指向它。

Thu Apr 28 22:06:54 EDT 2016
*任何一个设备的根目录的dentry，都是内核虚拟出来的，因为已经是根目录了，它就不存在于任何一个父目录。 你可能觉得，它的父目录难道不该是”挂载点“的那个目录吗。不是，要知道，在把一个目录变成挂载点的时候，它作为一个directory，大多数属性是一动不动的，只是dentry->vfsmount加链入了一个vfsmount结构，内核也就因此不再访问它的content（在pathwalk的时候），但只要内核想，内核仍然能把它当做普通的目录进入。 它的ents里并没有那个虚拟的root dentry。它与这个small_root的联系，是通过vfsmount链表里的某个vfsmount->small_root的联系的。

Thu Apr 28 09:09:52 EDT 2016
*有时间要把utils.h分离出来一些，string.h。
 确实，大多数.c文件都需要包含string.h, list.h, valType.h，把它们都包含在一个头文件里，这样写c文件时候只需要包含一次就可以了。
 但是，第一，分离还是必要的。 第二，没有必要包含一个头文件时，一定不要包含它。这样干净。而且将来内核大了，总是改变头文件引起的重编译开销最大。第三，是不是用utils.h当这个wrapper，还是把utils.h也清理干净，称为一个纯粹的头文件。专门另外弄一个often.h。

Thu Apr 28 04:46:14 EDT 2016
*感觉linux的那些头文件也不合理。有dcache.h，为什么没有inode.h. struct dentry declared in dcache.h，struct inode却声明在fs.h。 感觉没必要记它的。

Mon Apr 25 23:28:39 EDT 2016
*i/shashtable_add/del/has，总觉得这几个接口很好，但hashtable_has这个接口必须用宏实现，而且应该需要7个参数，就显得没有意义了。add/del倒是可以考虑。而且不需要i/s分开。
但问题是，hashtable_add并不很必要:
hashtable_add(list_head* hashtable, int hash, list_head *toadd); 它的实现就两句代码，因为紧接着还要调用list_add。更关键的是。。不不不，可以考虑，这儿只是传一个hash value。虽然只是简单的包装，但对程序的可读性帮助很大。 由此也看出c的缺点，太基本太松散，一个逻辑要写一大堆代码，没办法用高级的函数实现。


Mon Apr 25 02:51:49 EDT 2016
*vfs该不该提供不缓存inode的选项？ 实现上是有可能的，像一条路径/home/wws/lab/，pathwalk一道之后，dentry都缓存起来了，下次根本就不经过inode。 但是就现有代码来说，对inode还是默认的硬性的依赖的，像比real_lookup的时候，会无条件的认为indir->dentry->inode是存在于内存的。
*iget 这个函数被提炼出来是很自然的，但在里面递增inode个count计数就不那么自然了。最好功能纯粹：就是告诉你设备号：inode号，你把它（从hashtable或者磁盘上)读出来。

Tue Apr 19 21:36:00 EDT 2016
*函数宁可长一些，不要学linux分级成太多的函数。那样会前者鼻子走，忘得很快。
*觉得不舒服肯定要原因，像刚才把hash的过程合并到一个函数，只是分离出一个通用的
str_hash函数，就好多了。并且把抄来的str_hash的算法看了一下，就感觉好多了。

Sun Mar 27 14:34:28 EDT 2016
*在block里添加自己的id的确重要。 好几处都遇到了。 而且通过内存中block的地址根本
无法算出来它的id，这个空间大概是省不得。

Sat Mar 26 12:28:24 EDT 2016
*linux的vfs真是清爽。

Thu Mar 24 12:04:09 EDT 2016
* 今天发现ring0的进程不能被正常schedule。
  因为kernel.asm里设置了“只在用户态发生的中断才能引起schedule",所以，一个ring0进程的time_slice虽然变成0，并进而被设置成need_resched=1,但从timer的doIRQ返回的时候，根本就是直接restore_all的，不会去ret_with_reschedule。 ring生成1就好了。
Thu Mar 24 06:24:05 EDT 2016
×papaya的代码要做到0warning。

Wed Mar 23 12:21:11 EDT 2016
*放弃对block layer的模拟，因为实在受不了那些数据接口，函数绕来绕去。
 reqeust queue里的do_request和make_reqeust指针很牵强。
 没有动力模仿下去，又没有能力做出判断。
 那就还是模仿0.11，只是从2.4里借鉴几个好的数据结构。以后都自己设计着写，看自动进化成什么样子。

Sun Jul  5 03:40:49 EDT 2015
１，双重页错误，可能是响应ｉｎｉｔ进程页错误时，在响应代码里访问了非法地址。
像比，数据段在0x8049000，但并没有隐射。
Thu Jul  2 02:04:55 PDT 2015
1，testbuf太大的话，会DOUBLE PAGE error。先调小ceil_read最后拷贝的size。
2,把testbuf从exter改成mm.c内的全局声明时就好了，似乎不是1的原因。
